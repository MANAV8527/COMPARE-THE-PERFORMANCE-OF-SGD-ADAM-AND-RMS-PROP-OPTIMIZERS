{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MANAV8527/COMPARE-THE-PERFORMANCE-OF-SGD-ADAM-AND-RMS-PROP-OPTIMIZERS/blob/main/DeepFake_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Importing Required Libraries**"
      ],
      "metadata": {
        "id": "vb2sVHINZ7nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ptmm_pH1aHjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Define Parameters and Video Paths**"
      ],
      "metadata": {
        "id": "uX1t4f1iaWPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 10\n",
        "FRAME_SIZE = (224, 224)\n",
        "OUTPUT_FOLDER = '/content/frames'\n",
        "\n",
        "video_paths_real = [\"/content/v1.mp4\", \"/content/v2.mp4\" , \"/content/v3.mp4\", \"/content/v4.mp4\", \"/content/v5.mp4\", \"/content/v6.mp4\", \"/content/v7.mp4\", \"/content/v8.mp4\", \"/content/v9.mp4\", \"/content/v10.mp4\", \"/content/v11.mp4\"]\n",
        "video_paths_fake = [\"/content/vs1.mp4\", \"/content/vs2.mp4\", \"/content/vs3.mp4\", \"/content/vs4.mp4\", \"/content/vs5.mp4\", \"/content/vs6.mp4\", \"/content/vs7.mp4\", \"/content/vs8.mp4\", \"/content/vs9.mp4\", \"/content/vs10.mp4\", \"/content/vs11.mp4\"]\n",
        "\n",
        "video_paths = video_paths_real + video_paths_fake\n",
        "\n",
        "for path in video_paths:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    else:\n",
        "        print(f\"[âœ“] Found: {path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEdNM0edad98",
        "outputId": "b9e12c83-d025-49ce-aacb-b9a90c04959e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[âœ“] Found: /content/v1.mp4\n",
            "[âœ“] Found: /content/v2.mp4\n",
            "[âœ“] Found: /content/v3.mp4\n",
            "[âœ“] Found: /content/v4.mp4\n",
            "[âœ“] Found: /content/v5.mp4\n",
            "[âœ“] Found: /content/v6.mp4\n",
            "[âœ“] Found: /content/v7.mp4\n",
            "[âœ“] Found: /content/v8.mp4\n",
            "[âœ“] Found: /content/v9.mp4\n",
            "[âœ“] Found: /content/v10.mp4\n",
            "[âœ“] Found: /content/v11.mp4\n",
            "[âœ“] Found: /content/vs1.mp4\n",
            "[âœ“] Found: /content/vs2.mp4\n",
            "[âœ“] Found: /content/vs3.mp4\n",
            "[âœ“] Found: /content/vs4.mp4\n",
            "[âœ“] Found: /content/vs5.mp4\n",
            "[âœ“] Found: /content/vs6.mp4\n",
            "[âœ“] Found: /content/vs7.mp4\n",
            "[âœ“] Found: /content/vs8.mp4\n",
            "[âœ“] Found: /content/vs9.mp4\n",
            "[âœ“] Found: /content/vs10.mp4\n",
            "[âœ“] Found: /content/vs11.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Extract Frames from Videos**"
      ],
      "metadata": {
        "id": "1P7EyMJvakrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames(video_path, output_folder):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_path = os.path.join(output_folder, f\"{frame_count}.jpg\")\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n"
      ],
      "metadata": {
        "id": "5q5c_u8pa0eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Preprocess Frames**"
      ],
      "metadata": {
        "id": "ut6duKIVbDLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_frames(frame_path):\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    img = cv2.imread(frame_path)\n",
        "    if img is None:\n",
        "        return []\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        resized = cv2.resize(img, FRAME_SIZE) / 255.0\n",
        "        return [resized]\n",
        "\n",
        "    cropped_faces = []\n",
        "    for (x, y, w, h) in faces:\n",
        "        face = img[y:y+h, x:x+w]\n",
        "        face = cv2.resize(face, FRAME_SIZE) / 255.0\n",
        "        cropped_faces.append(face)\n",
        "    return cropped_faces\n"
      ],
      "metadata": {
        "id": "kEFSJqK9bJPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Load Dataset (Frames + Labels)**"
      ],
      "metadata": {
        "id": "NYhdvGJvbNXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(video_paths, output_folder):\n",
        "    frames = []\n",
        "    labels = []\n",
        "    for video_path in video_paths:\n",
        "        video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "        video_output_folder = os.path.join(output_folder, video_name)\n",
        "        os.makedirs(video_output_folder, exist_ok=True)\n",
        "        extract_frames(video_path, video_output_folder)\n",
        "\n",
        "        frame_files = sorted([f for f in os.listdir(video_output_folder) if f.endswith('.jpg')])\n",
        "        for f in frame_files:\n",
        "            full_path = os.path.join(video_output_folder, f)\n",
        "            faces = preprocess_frames(full_path)\n",
        "            for face in faces:\n",
        "                frames.append(face)\n",
        "                label = 0 if \"v1\" in video_path or \"v2\" in video_path else 1\n",
        "                labels.append(label)\n",
        "    return np.array(frames), np.array(labels)\n"
      ],
      "metadata": {
        "id": "65CR_nQKbVL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Create Sequences**"
      ],
      "metadata": {
        "id": "lh7YVghHbXoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences_per_video(frames, labels, sequence_length):\n",
        "    sequences, sequence_labels = [], []\n",
        "    current_video, current_labels = [], []\n",
        "\n",
        "    for i in range(len(frames)):\n",
        "        current_video.append(frames[i])\n",
        "        current_labels.append(labels[i])\n",
        "\n",
        "        if (i + 1) % sequence_length == 0:\n",
        "            sequences.append(np.array(current_video))\n",
        "            sequence_labels.append(current_labels[sequence_length // 2])  # middle frame's label\n",
        "            current_video, current_labels = [], []\n",
        "\n",
        "    return np.array(sequences), np.array(sequence_labels)"
      ],
      "metadata": {
        "id": "gberEf4JbcAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Prepare Training and Validation Data**"
      ],
      "metadata": {
        "id": "pJcDTffYbfW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frames, labels = load_dataset(video_paths, OUTPUT_FOLDER)\n",
        "print(f\"Total frames: {len(frames)}, Labels: {len(labels)}\")\n",
        "\n",
        "frames, labels = shuffle(frames, labels, random_state=42)\n",
        "train_frames, val_frames, train_labels, val_labels = train_test_split(frames, labels, test_size=0.2, random_state=42)\n",
        "train_seq, train_seq_labels = create_sequences_per_video(train_frames, train_labels, SEQUENCE_LENGTH)\n",
        "val_seq, val_seq_labels = create_sequences_per_video(val_frames, val_labels, SEQUENCE_LENGTH)\n",
        "\n",
        "print(f\"Train shape: {train_seq.shape}, Validation shape: {val_seq.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlLmjLLnbqgW",
        "outputId": "d9adce73-7cc3-478f-c707-73bd754f2849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total frames: 2358, Labels: 2358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Build CNN-LSTM Model**"
      ],
      "metadata": {
        "id": "otxO2Y2wcXui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(SEQUENCE_LENGTH, 224, 224, 3)),\n",
        "    TimeDistributed(MaxPooling2D((2, 2))),\n",
        "    TimeDistributed(Conv2D(64, (3, 3), activation='relu')),\n",
        "    TimeDistributed(MaxPooling2D((2, 2))),\n",
        "    TimeDistributed(Flatten()),\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cf7fISSeccTO",
        "outputId": "750b1938-0a43-4fb5-ea4e-02d4e8cd34d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Sequential' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-625f2ed812c2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = Sequential([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9: Train the Model**"
      ],
      "metadata": {
        "id": "Rf7mZ_YaceoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(train_seq, train_seq_labels,\n",
        "                    validation_data=(val_seq, val_seq_labels),\n",
        "                    epochs=10,\n",
        "                    batch_size=8,\n",
        "                    callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "PTBrKOwbclAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10: Plot Accuracy and Loss Graph**"
      ],
      "metadata": {
        "id": "IeCnHXY7cntg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', color='red', linewidth=2)\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss', color='blue', linewidth=2)\n",
        "    plt.title(\"Loss Over Epochs\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='green', linewidth=2)\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='orange', linewidth=2)\n",
        "    plt.title(\"Accuracy Over Epochs\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n"
      ],
      "metadata": {
        "id": "xXH6qpEycuvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 11: Evaluate the Model**"
      ],
      "metadata": {
        "id": "Gr4Yuy8dcxlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(val_seq)\n",
        "preds_class = np.round(preds).flatten()\n",
        "acc = accuracy_score(val_seq_labels, preds_class)\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Accuracy: {acc:.2f}\")\n",
        "print(\"\\nðŸ“‹ Classification Report:\\n\", classification_report(val_seq_labels, preds_class))\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(true_labels, pred_labels):\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(val_seq_labels, preds_class)\n"
      ],
      "metadata": {
        "id": "NhGkgJ-Jc4Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 12: Show Sample Predictions**"
      ],
      "metadata": {
        "id": "dQZOCd3-c87p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_predictions(sequences, labels, preds, num_samples=5):\n",
        "    idxs = np.random.choice(len(sequences), num_samples, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        seq = sequences[idx]\n",
        "        label = labels[idx]\n",
        "        pred = int(preds[idx])\n",
        "\n",
        "        plt.figure(figsize=(12, 1))\n",
        "        for j in range(len(seq)):\n",
        "            plt.subplot(1, len(seq), j+1)\n",
        "            plt.imshow(seq[j])\n",
        "            plt.axis('off')\n",
        "        plt.suptitle(f\"True: {'Real' if label==0 else 'Fake'} | Predicted: {'Real' if pred==0 else 'Fake'}\",\n",
        "                     fontsize=12, color='green' if label==pred else 'red')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "show_sample_predictions(val_seq, val_seq_labels, preds_class)\n"
      ],
      "metadata": {
        "id": "BCBhJ7gAdA3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Mxd1Q6904ngp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}